{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601454fd-0a19-427b-8b4a-70c96d7257d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/22 04:49:10 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Spark Table') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86ed827-45ee-4e18-a474-b0ac862004b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-f3f6-m.us-central1-f.c.still-smithy-449618-m7.internal:36571\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f995ca856c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77539419-cf42-4c78-bcf4-f54401fe3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = '/ecommerce_data/ecommerce_data/300MB/customers.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bd5da07-c785-49c2-842d-b740e3bc4492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "     .format('csv') \\\n",
    "     .option('header', 'True') \\\n",
    "     .option('inferschema', 'True') \\\n",
    "     .load(hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98421c90-9ba3-46ee-9774-4c2a712dc90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+-----------+-------+-------------------+---------+\n",
      "|customer_id|       name|     city|      state|country|  registration_date|is_active|\n",
      "+-----------+-----------+---------+-----------+-------+-------------------+---------+\n",
      "|          0| Customer_0|     Pune|Maharashtra|  India|2023-01-19 00:00:00|     true|\n",
      "|          1| Customer_1|     Pune|West Bengal|  India|2023-08-10 00:00:00|     true|\n",
      "|          2| Customer_2|    Delhi|Maharashtra|  India|2023-08-05 00:00:00|     true|\n",
      "|          3| Customer_3|   Mumbai|  Telangana|  India|2023-06-04 00:00:00|     true|\n",
      "|          4| Customer_4|    Delhi|  Karnataka|  India|2023-03-15 00:00:00|    false|\n",
      "|          5| Customer_5|  Kolkata|West Bengal|  India|2023-08-19 00:00:00|     true|\n",
      "|          6| Customer_6|  Kolkata| Tamil Nadu|  India|2023-04-21 00:00:00|    false|\n",
      "|          7| Customer_7|   Mumbai|  Telangana|  India|2023-05-23 00:00:00|     true|\n",
      "|          8| Customer_8|     Pune| Tamil Nadu|  India|2023-07-17 00:00:00|     true|\n",
      "|          9| Customer_9|    Delhi|  Karnataka|  India|2023-06-02 00:00:00|     true|\n",
      "|         10|Customer_10|Hyderabad|      Delhi|  India|2023-02-23 00:00:00|     true|\n",
      "|         11|Customer_11|    Delhi|West Bengal|  India|2023-11-08 00:00:00|     true|\n",
      "|         12|Customer_12|    Delhi|      Delhi|  India|2023-06-27 00:00:00|    false|\n",
      "|         13|Customer_13|     Pune|Maharashtra|  India|2023-02-03 00:00:00|     true|\n",
      "|         14|Customer_14|  Chennai|  Karnataka|  India|2023-04-06 00:00:00|     true|\n",
      "|         15|Customer_15|Hyderabad|West Bengal|  India|2023-03-31 00:00:00|     true|\n",
      "|         16|Customer_16|  Chennai|Maharashtra|  India|2023-04-26 00:00:00|     true|\n",
      "|         17|Customer_17|     Pune|      Delhi|  India|2023-04-14 00:00:00|    false|\n",
      "|         18|Customer_18|  Chennai|Maharashtra|  India|2023-02-04 00:00:00|    false|\n",
      "|         19|Customer_19|  Chennai|  Karnataka|  India|2023-01-22 00:00:00|     true|\n",
      "+-----------+-----------+---------+-----------+-------+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeda90e-2863-4be3-b47e-f7861ffa7ab0",
   "metadata": {},
   "source": [
    "### Creating Temporary View (Session Based) in Apache Spark\n",
    "\n",
    "In Apache Spark, we can create temporary views in two ways:\n",
    "\n",
    "1. **`createTempView()`**\n",
    "2. **`createOrReplaceTempView()`**\n",
    "\n",
    "#### 1. **`createTempView()`**:\n",
    "- **Purpose**: It creates a temporary view with the specified name.\n",
    "- **Behavior**: If a temporary view with the same name already exists, this method will fail with an error. It does not overwrite or replace the existing view.\n",
    "- **Syntax**:\n",
    "  ```python\n",
    "  df.createTempView('customer')\n",
    "  ```\n",
    "#### 2. **`createOrReplaceTempView()`**:\n",
    "- **Purpose**: It creates a temporary view with the specified name, but it will replace the existing view if one already exists with the same name.\n",
    "- **Behavior**: If a temporary view with the same name exists, it replaces the existing view without any errors. If the view does not exist, it simply creates the new one.\n",
    "- **Syntax**:\n",
    "  ```python\n",
    "  df.createOrReplaceTempView('customer')\n",
    "  ```\n",
    "#### Deleting a Temporary View in Spark\n",
    "\n",
    "In Apache Spark, there isn't a direct method like `dropTempView()` to delete a temporary view. However, you can \"drop\" a temporary view by using the `spark.catalog.dropTempView()` method.\n",
    "\n",
    "### Syntax to delete (drop) a temporary view:\n",
    "```python\n",
    "spark.catalog.dropTempView('view_name')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37fd4421-2e2b-4fe9-90ea-5553682bab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createTempView('customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86dd16d7-e875-45fc-87e4-5f1e5fc6a6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+-----------+-------+-------------------+---------+\n",
      "|customer_id|      name|  city|      state|country|  registration_date|is_active|\n",
      "+-----------+----------+------+-----------+-------+-------------------+---------+\n",
      "|          0|Customer_0|  Pune|Maharashtra|  India|2023-01-19 00:00:00|     true|\n",
      "|          1|Customer_1|  Pune|West Bengal|  India|2023-08-10 00:00:00|     true|\n",
      "|          2|Customer_2| Delhi|Maharashtra|  India|2023-08-05 00:00:00|     true|\n",
      "|          3|Customer_3|Mumbai|  Telangana|  India|2023-06-04 00:00:00|     true|\n",
      "|          4|Customer_4| Delhi|  Karnataka|  India|2023-03-15 00:00:00|    false|\n",
      "+-----------+----------+------+-----------+-------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from customer limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e086b1b-67f7-499d-8e33-7519b1561cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec83965c-1507-420d-9fdd-12dda7c96f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+-----------+-------+-------------------+---------+\n",
      "|customer_id|      name|   city|      state|country|  registration_date|is_active|\n",
      "+-----------+----------+-------+-----------+-------+-------------------+---------+\n",
      "|          0|Customer_0|   Pune|Maharashtra|  India|2023-01-19 00:00:00|     true|\n",
      "|          1|Customer_1|   Pune|West Bengal|  India|2023-08-10 00:00:00|     true|\n",
      "|          2|Customer_2|  Delhi|Maharashtra|  India|2023-08-05 00:00:00|     true|\n",
      "|          3|Customer_3| Mumbai|  Telangana|  India|2023-06-04 00:00:00|     true|\n",
      "|          4|Customer_4|  Delhi|  Karnataka|  India|2023-03-15 00:00:00|    false|\n",
      "|          5|Customer_5|Kolkata|West Bengal|  India|2023-08-19 00:00:00|     true|\n",
      "|          6|Customer_6|Kolkata| Tamil Nadu|  India|2023-04-21 00:00:00|    false|\n",
      "|          7|Customer_7| Mumbai|  Telangana|  India|2023-05-23 00:00:00|     true|\n",
      "|          8|Customer_8|   Pune| Tamil Nadu|  India|2023-07-17 00:00:00|     true|\n",
      "|          9|Customer_9|  Delhi|  Karnataka|  India|2023-06-02 00:00:00|     true|\n",
      "+-----------+----------+-------+-----------+-------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from customer limit 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "06ebb82c-d2ba-487e-b331-ffd342ec3a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropTempView('customer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7ba459-ea97-4e45-bc0d-72c6ed97a5e0",
   "metadata": {},
   "source": [
    "### `createOrReplaceGlobalTempView` (Accessible across sessions)\n",
    "\n",
    "In Apache Spark, `createOrReplaceGlobalTempView` is similar to `createOrReplaceTempView`, but it has a key difference: global temporary views are available across all sessions in the Spark application, while temporary views are only available within the session that created them.\n",
    "\n",
    "#### Purpose:\n",
    "It creates a global temporary view with the specified name, or replaces it if a global view with the same name already exists. Global views are accessible across all Spark sessions in the same application.\n",
    "\n",
    "#### Behavior:\n",
    "If a global temporary view with the same name exists, it will be replaced. If it doesn't exist, it will be created.\n",
    "\n",
    "#### Scope:\n",
    "Global temporary views are available across all Spark sessions in the application. They are stored in a system database called `global_temp`, and their lifetime is tied to the Spark application, not the session.\n",
    "\n",
    "#### Syntax\n",
    "```python\n",
    "    df.createOrReplaceGlobalTempView('global_customer')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "222c6d63-a8b1-4275-86ce-e15123bad8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceGlobalTempView('global_customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d8b63e0-192f-41bf-972d-420bd5934869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+-----------+-------+-------------------+---------+\n",
      "|customer_id|      name|   city|      state|country|  registration_date|is_active|\n",
      "+-----------+----------+-------+-----------+-------+-------------------+---------+\n",
      "|          0|Customer_0|   Pune|Maharashtra|  India|2023-01-19 00:00:00|     true|\n",
      "|          1|Customer_1|   Pune|West Bengal|  India|2023-08-10 00:00:00|     true|\n",
      "|          2|Customer_2|  Delhi|Maharashtra|  India|2023-08-05 00:00:00|     true|\n",
      "|          3|Customer_3| Mumbai|  Telangana|  India|2023-06-04 00:00:00|     true|\n",
      "|          4|Customer_4|  Delhi|  Karnataka|  India|2023-03-15 00:00:00|    false|\n",
      "|          5|Customer_5|Kolkata|West Bengal|  India|2023-08-19 00:00:00|     true|\n",
      "|          6|Customer_6|Kolkata| Tamil Nadu|  India|2023-04-21 00:00:00|    false|\n",
      "|          7|Customer_7| Mumbai|  Telangana|  India|2023-05-23 00:00:00|     true|\n",
      "|          8|Customer_8|   Pune| Tamil Nadu|  India|2023-07-17 00:00:00|     true|\n",
      "|          9|Customer_9|  Delhi|  Karnataka|  India|2023-06-02 00:00:00|     true|\n",
      "+-----------+----------+-------+-----------+-------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from global_temp.global_customer limit 10').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c9040-3a5c-4f38-a0ca-5f8f61047fe3",
   "metadata": {},
   "source": [
    "### Step 3: Create a Persistent Table (Stored in Hive Metastore)\n",
    "\n",
    "In this step, we are working with a **persistent table** in Apache Spark, which is stored in the **Hive Metastore**. This type of table persists across Spark sessions and applications, meaning it remains accessible even after the session ends. The data is stored in an external storage system (such as HDFS or S3), and it can be queried at any time.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "1. **Creating the Persistent Table**:\n",
    "   - A **persistent table** is created using the `saveAsTable()` method, which writes the DataFrame into a table format and registers it in the Hive Metastore.\n",
    "   - By default, the table is **managed**, meaning Spark manages both the **data** and **metadata**.\n",
    "   - The data for the table is stored externally (e.g., on HDFS, S3), and the metadata (such as schema) is stored in the Hive Metastore.\n",
    "   - Symtax\n",
    "   ``` python\n",
    "       df.write.mode(\"write\").saveAsTable(\"customers_persistent\")\n",
    "   ```\n",
    "\n",
    "2. **Querying the Persistent Table**:\n",
    "   - Once the table is created, it can be queried using SQL, and the data can be accessed even after restarting the Spark session.\n",
    "   - This demonstrates that the table is **persistent**, meaning it survives across sessions and is accessible by multiple Spark applications.\n",
    "   - Syntax\n",
    "   ``` python\n",
    "       spark.sql(\"SELECT * FROM customers_persistent LIMIT 5\").show()\n",
    "   ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7cf4dd2f-dfa3-4589-8682-5077617ea969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b423647a-5bd3-4bdf-b50c-db2c37b83f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('use default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20152f7c-6ef6-4a02-ae3f-f39b98e46ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/22 06:20:57 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider csv. Persisting data source table `default`.`customer` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    }
   ],
   "source": [
    "df.write.format('csv').saveAsTable('default.customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17792c53-a6ab-47bf-8687-6348354c01be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+-----------+-------+-------------------+---------+\n",
      "|customer_id|      name|   city|      state|country|  registration_date|is_active|\n",
      "+-----------+----------+-------+-----------+-------+-------------------+---------+\n",
      "|          0|Customer_0|   Pune|Maharashtra|  India|2023-01-19 00:00:00|     true|\n",
      "|          1|Customer_1|   Pune|West Bengal|  India|2023-08-10 00:00:00|     true|\n",
      "|          2|Customer_2|  Delhi|Maharashtra|  India|2023-08-05 00:00:00|     true|\n",
      "|          3|Customer_3| Mumbai|  Telangana|  India|2023-06-04 00:00:00|     true|\n",
      "|          4|Customer_4|  Delhi|  Karnataka|  India|2023-03-15 00:00:00|    false|\n",
      "|          5|Customer_5|Kolkata|West Bengal|  India|2023-08-19 00:00:00|     true|\n",
      "|          6|Customer_6|Kolkata| Tamil Nadu|  India|2023-04-21 00:00:00|    false|\n",
      "|          7|Customer_7| Mumbai|  Telangana|  India|2023-05-23 00:00:00|     true|\n",
      "|          8|Customer_8|   Pune| Tamil Nadu|  India|2023-07-17 00:00:00|     true|\n",
      "|          9|Customer_9|  Delhi|  Karnataka|  India|2023-06-02 00:00:00|     true|\n",
      "+-----------+----------+-------+-----------+-------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from default.customer limit 10').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0cbc1-6b35-4a8b-ab81-5937e71bc975",
   "metadata": {},
   "source": [
    "# Persistent Table in Hive Metastore\n",
    "\n",
    "## Overview\n",
    "\n",
    "A **Persistent Table** in Apache Spark refers to a table whose **data** and **metadata** are stored in the **Hive Metastore**. The table persists beyond the Spark session, meaning it remains available until explicitly dropped. These tables can be accessed across multiple Spark sessions and even across different applications.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Data and Metadata:\n",
    "- Both the **data** and **metadata** (schema) are stored in the **Hive Metastore**.\n",
    "- The table persists even after the Spark session ends, making it accessible beyond a single session.\n",
    "- The metadata, including the schema, is stored in the Hive Metastore, while the data is typically stored in an external storage system (e.g., HDFS, S3).\n",
    "\n",
    "---\n",
    "\n",
    "## Managed vs External Tables\n",
    "\n",
    "### 1. Managed Table:\n",
    "- **Data & Metadata**: Spark manages both the **metadata** and the **data**.\n",
    "- **Behavior on Drop**: If you drop a managed table, both the metadata and the data are deleted.\n",
    "- **Example**: A managed table is typically created when you use `saveAsTable()` without specifying a storage location.\n",
    "\n",
    "### 2. External Table:\n",
    "- **Metadata Only**: In an external table, only the **metadata** is stored in the Hive Metastore, while the **data** is stored externally (e.g., in HDFS, S3).\n",
    "- **Behavior on Drop**: Dropping an external table only removes the metadata from the Hive Metastore. The actual data in the external storage is not deleted.\n",
    "- **Example**: An external table is created when you specify a `LOCATION` for the table data.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points\n",
    "\n",
    "- **Persistence**: Persistent tables are available across sessions and Spark applications, ensuring that they remain accessible for long-term use.\n",
    "- **Flexibility**: You can create either **managed** or **external** tables depending on how you want to handle the table's data and metadata.\n",
    "- **Hive Metastore**: The metadata of the table is stored in the Hive Metastore, making it accessible to other tools and Spark applications that can read from the Metastore.\n",
    "  \n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Persistent Tables** are stored in the Hive Metastore and can be queried across multiple Spark sessions.\n",
    "- **Managed Tables**: Both data and metadata are managed by Spark. Dropping a managed table removes both the data and metadata.\n",
    "- **External Tables**: Only metadata is managed by Spark, while the data is stored externally. Dropping an external table removes only the metadata, leaving the data intact.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f8e58e-6a83-423a-bfe7-fd124c6ae6fe",
   "metadata": {},
   "source": [
    "### **Managed Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd2b8a35-996a-4dcc-8d16-c8d8df64ed9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ce97421-0feb-4397-af5d-134cb53cef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+--------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                         |comment|\n",
      "+----------------------------+--------------------------------------------------+-------+\n",
      "|customer_id                 |int                                               |null   |\n",
      "|name                        |string                                            |null   |\n",
      "|city                        |string                                            |null   |\n",
      "|state                       |string                                            |null   |\n",
      "|country                     |string                                            |null   |\n",
      "|registration_date           |timestamp                                         |null   |\n",
      "|is_active                   |boolean                                           |null   |\n",
      "|                            |                                                  |       |\n",
      "|# Detailed Table Information|                                                  |       |\n",
      "|Database                    |default                                           |       |\n",
      "|Table                       |customer                                          |       |\n",
      "|Owner                       |root                                              |       |\n",
      "|Created Time                |Sat Feb 22 06:20:57 UTC 2025                      |       |\n",
      "|Last Access                 |UNKNOWN                                           |       |\n",
      "|Created By                  |Spark 3.3.2                                       |       |\n",
      "|Type                        |MANAGED                                           |       |\n",
      "|Provider                    |csv                                               |       |\n",
      "|Statistics                  |417334215 bytes                                   |       |\n",
      "|Location                    |hdfs://cluster-f3f6-m/user/hive/warehouse/customer|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe|       |\n",
      "+----------------------------+--------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe extended customer').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db4ae6",
   "metadata": {},
   "source": [
    "# **Before dropping the Managed table**\n",
    "\n",
    "**Path:** `/user/hive/warehouse/customer`\n",
    "\n",
    "---\n",
    "\n",
    "### Directory Content\n",
    "\n",
    "| Permission | Owner | Group | Size       | Last Modified | Replication | Block Size | Name                                             |\n",
    "|------------|-------|-------|------------|---------------|-------------|------------|-------------------------------------------------|\n",
    "| -rw-r--r-- | root  | hadoop| 0 B        | Feb 22 11:50  | 2           | 128 MB     | _SUCCESS                                        |\n",
    "| -rw-r--r-- | root  | hadoop| 155.87 MB  | Feb 22 11:50  | 2           | 128 MB     | part-00000-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv |\n",
    "| -rw-r--r-- | root  | hadoop| 155.42 MB  | Feb 22 11:50  | 2           | 128 MB     | part-00001-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv |\n",
    "| -rw-r--r-- | root  | hadoop| 86.71 MB   | Feb 22 11:50  | 2           | 128 MB     | part-00002-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv |\n",
    "\n",
    "\n",
    "**Note : Please image section for complete details refer to screenshots**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c41f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS customer\")\n",
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a471b4c",
   "metadata": {},
   "source": [
    "# **After dropping Managed table**\n",
    "**Path:** `/user/hive/warehouse/customer`\n",
    "\n",
    "---\n",
    "\n",
    "### Directory Content\n",
    "\n",
    "| Permission | Owner | Group | Size       | Last Modified | Replication | Block Size | Name          |\n",
    "|------------|-------|-------|------------|---------------|-------------|------------|-------------------------------------------------|\n",
    "||   | | |  ||       |                                         |                 \n",
    "\n",
    "**Note : Please image section for complete details refer to screenshots**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13721837-5c73-41cf-b407-70f8a3077a17",
   "metadata": {},
   "source": [
    "# **External Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a247bc9f-92ba-43c1-a861-04bebce61393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/22 06:13:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"External Table Example\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10c87071-f7c7-44f3-9747-14b5e17ec14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/22 06:24:04 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider CSV. Persisting data source table `default`.`customers_external` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS customers_external (\n",
    "    customer_id INT,\n",
    "    name STRING,\n",
    "    city STRING,\n",
    "    state STRING,\n",
    "    country STRING,\n",
    "    registration_date TIMESTAMP,\n",
    "    is_active BOOLEAN\n",
    ")\n",
    "USING CSV\n",
    "LOCATION '/user/hive/warehouse/customer/part-00000-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79a3849a-5ac7-4ca2-b8df-2ced4e7dc970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                                  |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------------------------------------------+-------+\n",
      "|customer_id                 |int                                                                                                        |null   |\n",
      "|name                        |string                                                                                                     |null   |\n",
      "|city                        |string                                                                                                     |null   |\n",
      "|state                       |string                                                                                                     |null   |\n",
      "|country                     |string                                                                                                     |null   |\n",
      "|registration_date           |timestamp                                                                                                  |null   |\n",
      "|is_active                   |boolean                                                                                                    |null   |\n",
      "|                            |                                                                                                           |       |\n",
      "|# Detailed Table Information|                                                                                                           |       |\n",
      "|Database                    |default                                                                                                    |       |\n",
      "|Table                       |customers_external                                                                                         |       |\n",
      "|Owner                       |root                                                                                                       |       |\n",
      "|Created Time                |Sat Feb 22 06:24:04 UTC 2025                                                                               |       |\n",
      "|Last Access                 |UNKNOWN                                                                                                    |       |\n",
      "|Created By                  |Spark 3.3.2                                                                                                |       |\n",
      "|Type                        |EXTERNAL                                                                                                   |       |\n",
      "|Provider                    |CSV                                                                                                        |       |\n",
      "|Location                    |hdfs://cluster-f3f6-m/user/hive/warehouse/customer/part-00000-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                                                         |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                                                           |       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe extended customers_external').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdcbaad",
   "metadata": {},
   "source": [
    "# **Before dropping the external table**\n",
    "\n",
    "**Path:** `/user/hive/warehouse/customer/part-00000-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv`\n",
    "\n",
    "---\n",
    "\n",
    "### Directory Content\n",
    "\n",
    "| Permission | Owner | Group | Size       | Last Modified | Replication | Block Size | Name                                             |\n",
    "|------------|-------|-------|------------|---------------|-------------|------------|-------------------------------------------------|\n",
    "| -rw-r--r-- | root  | hadoop| 0 B        | Feb 22 11:50  | 2           | 128 MB     | _SUCCESS                                        |\n",
    "| -rw-r--r-- | root  | hadoop| 155.87 MB  | Feb 22 11:50  | 2           | 128 MB     | part-00000-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv |\n",
    "| -rw-r--r-- | root  | hadoop| 155.42 MB  | Feb 22 11:50  | 2           | 128 MB     | part-00001-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv |\n",
    "| -rw-r--r-- | root  | hadoop| 86.71 MB   | Feb 22 11:50  | 2           | 128 MB     | part-00002-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv |\n",
    "\n",
    "**Note : Please image section for complete details refer to screenshots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "803b3b18-67b8-4cd5-b9b8-1e3fb6bb0852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS customers_external\")\n",
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **After dropping the external table**\n",
    "\n",
    "**Path:** `/user/hive/warehouse/customer/part-00000-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv`\n",
    "\n",
    "---\n",
    "\n",
    "### Directory Content\n",
    "\n",
    "| Permission | Owner | Group | Size       | Last Modified | Replication | Block Size | Name                                             |\n",
    "|------------|-------|-------|------------|---------------|-------------|------------|-------------------------------------------------|\n",
    "| -rw-r--r-- | root  | hadoop| 0 B        | Feb 22 11:50  | 2           | 128 MB     | _SUCCESS                                        |\n",
    "| -rw-r--r-- | root  | hadoop| 155.87 MB  | Feb 22 11:50  | 2           | 128 MB     | part-00000-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv |\n",
    "| -rw-r--r-- | root  | hadoop| 155.42 MB  | Feb 22 11:50  | 2           | 128 MB     | part-00001-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv |\n",
    "| -rw-r--r-- | root  | hadoop| 86.71 MB   | Feb 22 11:50  | 2           | 128 MB     | part-00002-41449e73-7692-40cc-986d-729e1a7a662f-c000.csv |\n",
    "\n",
    "**Note : Please image section for complete details refer to screenshots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52f7c51b-fa7d-4dee-b580-ec22a2a27655",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
